import os
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers, Model, Input, optimizers

# Dataset directories
data_dir = "mnist-jpg"
img_size = (28, 28)
color_mode = "grayscale"

train_dir = os.path.join(data_dir, "train")
test_dir = os.path.join(data_dir, "test")

# Load datasets
train_ds = keras.preprocessing.image_dataset_from_directory(
    train_dir, labels="inferred", label_mode="int",
    image_size=img_size, color_mode=color_mode,
    batch_size=128, shuffle=True, seed=42
)

test_ds = keras.preprocessing.image_dataset_from_directory(
    test_dir, labels="inferred", label_mode="int",
    image_size=img_size, color_mode=color_mode,
    batch_size=128, shuffle=False
)

# Convert to numpy arrays
def ds_to_numpy(dataset):
    imgs, labels = [], []
    for batch in dataset:
        b_x, b_y = batch
        imgs.append(b_x.numpy())
        labels.append(b_y.numpy())
    return np.vstack(imgs), np.hstack(labels)

x_train, y_train = ds_to_numpy(train_ds)
x_test, y_test = ds_to_numpy(test_ds)

# Remove extra channel dimension
x_train = x_train[..., 0]
x_test = x_test[..., 0]

# Normalize pixel values
x_train = x_train.astype("float32") / 255.0
x_test = x_test.astype("float32") / 255.0

# Build FFNN model
inp = Input(shape=x_train.shape[1:], name="input_image")
x = layers.Flatten(name="flatten")(inp)
x = layers.Dense(128, activation="relu", name="dense1")(x)
x = layers.Dense(64, activation="relu", name="dense2")(x)
out = layers.Dense(10, activation="softmax", name="preds")(x)

model = Model(inputs=inp, outputs=out, name="ffnn_mnistjpg")

# Compile model
sgd = optimizers.SGD(learning_rate=0.01, momentum=0.0)
model.compile(optimizer=sgd,
              loss="sparse_categorical_crossentropy",
              metrics=["accuracy"])

# Train model
history = model.fit(
    x_train, y_train,
    epochs=10,
    batch_size=128,
    validation_split=0.1,
    verbose=2
)

# Evaluate on test set
test_loss, test_acc = model.evaluate(x_test, y_test, verbose=0)
print(f"\nTest loss: {test_loss:.4f} | Test accuracy: {test_acc:.4f}")

# Plot training results
plt.figure(figsize=(12, 4))

plt.subplot(1, 2, 1)
plt.plot(history.history["loss"], label="train loss")
plt.plot(history.history["val_loss"], label="val loss")
plt.title("Loss")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(history.history["accuracy"], label="train acc")
plt.plot(history.history["val_accuracy"], label="val acc")
plt.title("Accuracy")
plt.xlabel("Epoch")
plt.ylabel("Accuracy")
plt.legend()

plt.tight_layout()
plt.show()




import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten, Input
from tensorflow.keras.optimizers import SGD
from tensorflow.keras.datasets import mnist
import matplotlib.pyplot as plt

# Load dataset
(x_train, y_train), (x_test, y_test) = mnist.load_data()

# Normalize input images
x_train = x_train.astype('float32') / 255.0
x_test = x_test.astype('float32') / 255.0

# One-hot encode labels
y_train = tf.keras.utils.to_categorical(y_train, 10)
y_test = tf.keras.utils.to_categorical(y_test, 10)

# Build FFNN model
model = Sequential([
    Input(shape=(28, 28)),
    Flatten(),
    Dense(128, activation='relu'),
    Dense(64, activation='relu'),
    Dense(10, activation='softmax')
])

# Compile model
model.compile(
    optimizer=SGD(learning_rate=0.01),
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

# Train model
history = model.fit(
    x_train, y_train,
    epochs=20,
    validation_data=(x_test, y_test)
)

# Evaluate on test data
test_loss, test_acc = model.evaluate(x_test, y_test)
print(f"Test Loss : {test_loss}")
print(f"Test Accuracy : {test_acc}")

# Plot loss and accuracy
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.plot(history.history['loss'], label="Training Loss")
plt.plot(history.history['val_loss'], label="Validation Loss")
plt.title("Loss vs Epochs")
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(history.history['accuracy'], label="Training Accuracy")
plt.plot(history.history['val_accuracy'], label="Validation Accuracy")
plt.title("Accuracy vs Epochs")
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()

plt.show()



1. Introduction
A Feedforward Neural Network (FFNN) is the simplest type of Artificial Neural Network
(ANN).â€‹
It consists of neurons (nodes) arranged in layers â€” where data flows only in one
direction,â€‹
from input â†’ hidden layers â†’ output, with no feedback connections.
âš™ï¸ 2. Structure of FFNN
1.â€‹ Input Layer:â€‹
â—‹â€‹ Takes the input features (e.g., pixel values, sensor readings, etc.).â€‹
â—‹â€‹ Passes them to the next layer.â€‹
2.â€‹ Hidden Layers:â€‹
â—‹â€‹ Perform intermediate computations.â€‹
â—‹â€‹ Each neuron applies a weighted sum and activation function to introduce
non-linearity.â€‹
3.â€‹ Output Layer:â€‹
â—‹â€‹ Produces the final result (e.g., class label or numeric value).â€‹
ğŸ§® 3. Mathematical Model
For each neuron:
z=âˆ‘i=1n(wixi)+bz = \sum_{i=1}^{n} (w_i x_i) + bz=i=1âˆ‘nâ€‹(wiâ€‹xiâ€‹)+b y=f(z)y = f(z)y=f(z)
Where:
â—â€‹ xix_ixiâ€‹: input featuresâ€‹
â—â€‹ wiw_iwiâ€‹: weightsâ€‹
â—â€‹ bbb: biasâ€‹â—â€‹ f(z)f(z)f(z): activation function (e.g., ReLU, sigmoid)â€‹
â—â€‹ yyy: output of the neuronâ€‹
4. Working Process
1.â€‹ Forward Propagation:â€‹
â—‹â€‹ Input data passes through the network layer by layer.â€‹
â—‹â€‹ Each neuron computes its activation value.â€‹
2.â€‹ Loss Calculation:â€‹
â—‹â€‹ The difference between the predicted output and actual output is calculated
using a loss function.â€‹
3.â€‹ Backward Propagation (Backpropagation):â€‹
â—‹â€‹ The loss is propagated backward.â€‹
â—‹â€‹ The model updates weights using Gradient Descent or similar optimization
techniques.â€‹
4.â€‹ Iteration:â€‹
â—‹â€‹ Steps 1â€“3 repeat for several epochs until loss is minimized.â€‹10. Advantages of FFNN
â—â€‹ Simple and easy to implementâ€‹
â—â€‹ Works well for structured dataâ€‹
â—â€‹ Forms the base of advanced networks (CNNs, RNNs, etc.)â€‹
âš ï¸ 11. Limitations
â—â€‹ Cannot handle sequential or spatial data directlyâ€‹
â—â€‹ Prone to overfitting on small datasetsâ€‹
â—â€‹ Requires large data for good accuracyâ€‹
ğŸ§¾ 12. Conclusion
Feedforward Neural Networks are the foundation of deep learning.â€‹
They learn complex relationships between input and output using forward propagation
and backpropagation.â€‹
The MNIST experiment demonstrates their ability to classify handwritten digits with high
accuracy.
